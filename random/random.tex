\documentclass{article}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{subcaption} 
\usepackage{cancel}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{indentfirst}

\usepackage[a4paper,
            bindingoffset=0.0in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1.3in,
            footskip=.5in]{geometry}

\title{Final Report: The Diverse Deal}
% \author{Pavan Yeddanapudi}
\date{\today}

\author{
  Yeddanapudi, Pavan\\
  \texttt{pky@mit.edu}
  \and
  Melendez, Armando\\
  \texttt{mondoj@mit.edu}
  \and
  Stewart, Wilson\\
  \texttt{wcs312@mit.edu}
}

\doublespacing
\begin{document}

    \maketitle

    \section*{Initial Bot}
    
    Our initial approach to designing a poker bot was more simple. With our team separated for the first week, our aim was to create a foundation for more advanced strategies in later iterations. We wanted a bot that could make rational decisions given the game conditions.
    
    \subsection*{Calling Preflop}
    
    For example, a key decision in our first bot was to avoid folding preflop. This was driven by two primary considerations: the relatively low cost of blinds and the potential value of hitting a strong hand that matched with our bounty. With a large starting stack of two-hundred big blinds, we figured a call of one chip would be worth it as the small blind. In doing so, we also gave ourselves a chance to see the flop and potentially connect with the board or our bounty, turning a seemingly weak hand into a strong one. While this decision was later modified and ultimately proved to contain issues (for example, allowing the opponents to connect with their bounties), it was also a good way to obtain more data by observing game logs to figure out ways to improve down the road.
    
    \subsection*{Calculating Equity}
    Post flop, the backbone of the bot’s decision-making process was equity calculation, measuring the probability of the bot’s hand winning against a range of potential opponent hands. To estimate equity, we implemented a Monte Carlo simulation. The simulation generated thousands of random outcomes based on the bot’s current hand and visible community cards. 
    Once equity was calculated, the bot’s actions were determined based on a predefined threshold. If the estimated equity exceeded a certain bound, the bot would either check (if possible) or call the current bet. This ensured that the bot stayed in the game when it had a reasonable chance of success. However, if the equity fell below the threshold, the bot folded. While this decision framework was straightforward, it provided a solid starting point to evolve our strategy in later iterations.

    \section*{Next Iteration}

    Mandobot was our second major exploration into poker strategy before we moved on to more advanced solutions. While it didn’t reach the level of theoretical depth that our final CFR-based bot did, it played an important role in our learning process and taught us several crucial lessons about integrating opponent modeling and equity calculations.
    
    \subsection*{Positional Understanding}
    
    At a high level, Mandobot relied on a naive equity estimate to guide its betting decisions. To do this, we ran a simple Monte Carlo simulation: we randomly assigned possible hole cards to the opponent and “played out” the remaining community cards to see how often our hand would come out on top. This gave us a quick-and-dirty probability of winning. Although not perfect, it gave Mandobot a good-enough snapshot of its strength in many situations.

    Beyond equity, we introduced opponent modeling by tracking how often the opponent folded after we raised. If the opponent rarely called raises, Mandobot seized the opportunity to bluff more aggressively, hoping to pick up smaller pots without a fight. Conversely, if the opponent was stubborn and called more often, Mandobot dialed back the bluffs. This small adaptation helped us understand how adjusting to opponents’ habits can impact our bot’s performance significantly.

    \subsection*{Bounty}

    Another unique feature we put more emphasis on was the bounty rank. If a card with that bounty rank was in our hand or on the board, we added a small bonus to our equity. Even if it was still hiding somewhere in the deck, we added a smaller bonus. Although this was somewhat contrived for our course-specific scenario, it demonstrated how external objectives (like side missions or special points) can be factored into a bot’s decisions. When Mandobot decided to raise, it typically used a size proportional to our equity times the pot size—so if we felt really good about our chances, we raised bigger, but if we were only moderately confident (or bluffing), we stuck to smaller bets. While not as elegant or theoretically sound as our CFR approach, it still performed decently in many test matches.

    Overall, Mandobot was a valuable stepping stone in our journey. It confirmed that even a relatively simple approach—when combined with basic opponent modeling and a dash of creativity—can hold its own under the right conditions

    \pagebreak
    
    \section*{Final Bot}
   
    \subsection*{CFR Algorithm}
    Our final bot was a combination of all the things we have learned in the class and from our previous installations. We decided to implement the CFR algorithm which has been shown to be one of the most optimal solutions to poker. The idea behind it is to begin with a game tree as shown in Figure 1. 

    \begin{wrapfigure}{L}{0.4\textwidth}
    \centering
    \captionsetup{justification=centering,margin=2cm}
    \includegraphics[width=0.4\textwidth]{game_tree.png}
    \caption{\label{fig:tree_diagram}}
    \end{wrapfigure}

    Each node represents the state of the game and each edge represents a possible action that is available to use. The leaf nodes at the bottom of the graph represent the terminal state payoffs for the players. Using this tree, we want to see the utility, or how happy we are, at each decision point. This is where the CFR algorithm comes into to play. We simulate the game play and update the regret using the following formula, 

    \[R_t(a) = U(a) - U(s_t)\]

    where $a$ is the action and $s_t$ is the current action we have decided on based on prior training. 
    
    Using this, we update the overall action probabilities using an additional formula, 

    \[\sigma_t(a) = \frac{\max{(R_t(a),0)}}{\sum_{a'}\max{(R_t(a),0)}}\]

    As a result, we now have the basic overview needed for improving the performance of our bot. We then use this as a probability distribution for every decision node in the tree, using the python random generator choose our decision. 

    \subsection*{Aggression}

    However, this was not enough as there are additional challenges we needed to overcome. We also needed a method of how to choose how much our bot should raise when playing and we devised the following method. We would have a value that represent our base aggression, which we decided to be half of the value of our equity. From there, we would also be tracking how aggressive our opponents were playing, which was basically the fraction of how many times they decided to bluff over the total number of rounds. If they were playing aggressively, then we decided that we should be aggressive as well, as our opponents had been bluffing too much. Using this fraction, we would multiply this with the pot size and use this raise value whenever our bot decided to raise based on the CFR weights. 
    
    To ``train'' our bot, we decided to set two of these CFR bots against each other and let them simulate a number of hands, letting them train each other and them updating the same file during the training process. This idea was based upon the original CFR paper and they decided to train it. 

    Afterwards, we had to experiment with different parameters. For example, during out training, we found that the bot wasn't being nearly aggressiveness in some situations. We reasoned this out to be because there are too many game states in poker and as a result, during our training, not every step of the game tree was getting reach multiple times which could lead to a bias in the way it plays. Since in poker, you will lose on a hand more often than you win, we decided that it was biased in that direction. To combat this we decided to hard code some default aggression in our bot, making it call and raise a bit more often so that it would actually play more hands.

    Overall, this bot outperformed all our previous iteration by a large margin during training. It was able to win by a large margin each time, upwards of 600 or more during each test. 

    \section*{Team Contributions}

    In terms of team contributions, we split the tasks among members who focused on the Monte Carlo simulation, the bounty rank logic, and the basic “bluff EV” calculations. Putting it all together helped us develop a deeper understanding of fundamental poker concepts like pot odds and fold equity. More importantly, it gave us practice in spotting straightforward ways to exploit patterns in an opponent’s play—a skill we eventually refined and expanded on when building our final CFR bot.

    \section*{Takeaways}

    This was an interesting problem, seeing how to teach a computer to outperform other computers in a game as complicated as poker. The main takeaway we had as a team is solving these types of games are more math than anything else. The way computers think about solving these types of games are so much different than humans as we don't have as much of a mathematical formulation behind the decisions we make. Our ``training" process comes through extensive practice and thought, not really updating any parameters, but getting an abstract ``feel" for the game and how we like to play it. To understand the game in a more mathematical sense lets us get an entirely new perspective and will allow us to formulate a more mathematically rigorous idea when we attack new problems.

    
\end{document}
